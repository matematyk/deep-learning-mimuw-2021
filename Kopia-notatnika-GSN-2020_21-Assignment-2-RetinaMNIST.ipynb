{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopia notatnika GSN 2020/21 Assignment 2: RetinaMNIST","provenance":[{"file_id":"1Gr9TjrFNcsCwGdx0xQueuaAwRwSJeqOA","timestamp":1621274737437},{"file_id":"1UacrA6FVa9t2UaqskbjK0Pu7jii6aNPF","timestamp":1620894074365},{"file_id":"1__FRfHVgYjcwqc34WtbKzj4yrnq3_x3A","timestamp":1620147125646},{"file_id":"1A2vsHs1AZXBl31A3BEqT_ohN4x7sNI8G","timestamp":1620136911779}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6VBJpAMzglRo"},"source":["# RetinaMNIST\n","\n","In this exercise your goal will be to solve an object detection training and prediction task using the anchor-based approach.\n","**As a part of your solution you should provide a report summarizing your findings and results of the conducted experiments.**\n","\n","##TLDR; overview\n","\n","In this task one should:\n","- determine the size of the feasible anchors for the object detection task posed in this Assignment,\n","- build an object detection model using the variant of `RetinaNet`,\n","- prepare a matching suite which will match predicted anchors with ground truth bounding boxes,\n","- train an object detection model using a variant of `RetinaLoss`.\n","\n","Hints and comments:\n","\n","- [ ]  Model architecture and loss are heavily inspired by [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf) paper,\n","- you can freely subclass and extend the interface of classes in this exercise,\n","- [ ] be sure that you understand the concept of an anchor for object detection, covered during Lecture 8. There are many great tutorials and articles about it (e.g. [this](https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9) one, note however that we are not implementing ignoring boxes for simplicity).\n","\n","### Data description\n","\n","In this task we will paste bounding boxes with digits randomly selected from `MNIST` dataset on a canvas of size `(128, 128)`. We assume that:\n","\n","- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n","- the digits are fully contained in canvas,\n","- boxes are modeled using `MnistBox` class,\n","- canvas is modeled using `MnistCanvas` class.\n","\n","Let us have a look at definition of these classes:"]},{"cell_type":"code","metadata":{"id":"L1rAdIiRq2G8","executionInfo":{"status":"ok","timestamp":1621284035016,"user_tz":-120,"elapsed":991,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["from typing import List\n","from typing import Optional\n","from typing import Tuple\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","import torch\n","\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class MnistBox:\n","\n","    def __init__(\n","        self,\n","        x_min: int,\n","        y_min: int,\n","        x_max: int,\n","        y_max: int,\n","        class_nb: Optional[int] = None,\n","    ):\n","        self.x_min = x_min\n","        self.x_max = x_max\n","        self.y_min = y_min\n","        self.y_max = y_max\n","        self.class_nb = class_nb\n","    \n","    @property\n","    def x_diff(self):\n","        return self.x_max - self.x_min\n","    \n","    @property\n","    def y_diff(self):\n","        return self.y_max - self.y_min\n","\n","    def __repr__(self):\n","        return f'Mnist Box: x_min = {self.x_min},' +\\\n","               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n","               f' y_max = {self.y_max}. Class = {self.class_nb}'\n","\n","    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n","        ax.add_patch(\n","            patches.Rectangle(\n","                (self.y_min, self.x_min),\n","                 self.y_diff,\n","                 self.x_diff,\n","                 linewidth=1,\n","                 edgecolor=color,\n","                 facecolor='none',\n","            )\n","        )\n","        ax.text(\n","            self.y_min,\n","            self.x_min,\n","            str(self.class_nb),\n","            bbox={\"facecolor\": color, \"alpha\": 0.4},\n","            clip_box=ax.clipbox,\n","            clip_on=True,\n","        )\n","\n","    @property\n","    def area(self):\n","        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n","\n","    def iou_with(self, other_box: \"MnistBox\"):\n","        aux_box = MnistBox(\n","            x_min=max(self.x_min, other_box.x_min),\n","            x_max=min(self.x_max, other_box.x_max),\n","            y_min=max(self.y_min, other_box.y_min),\n","            y_max=min(self.y_max, other_box.y_max),\n","        ) \n","        return aux_box.area / (self.area + other_box.area - aux_box.area)\n","\n","\n","class MnistCanvas:\n","\n","    def __init__(\n","        self,\n","        image: np.ndarray,\n","        boxes: List[MnistBox],\n","    ):\n","        self.image = image\n","        self.boxes = boxes\n","\n","    def add_digit(\n","        self,\n","        digit: np.ndarray,\n","        class_nb: int,\n","        x_min: int,\n","        y_min: int,\n","        iou_threshold=0.1,\n","    ) -> bool:\n","        \"\"\"\n","        Add a digit to an image if it does not overlap with existing boxes\n","        above iou_threshold.\n","        \"\"\"\n","        image_x, image_y = digit.shape\n","        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n","            raise ValueError('Wrong initial corner box')\n","        new_box_x_min = x_min\n","        new_box_y_min = y_min\n","        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n","        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n","        new_box = MnistBox(\n","            x_min=new_box_x_min,\n","            x_max=new_box_x_max,\n","            y_min=new_box_y_min,\n","            y_max=new_box_y_max,\n","           class_nb=class_nb,\n","        )\n","        old_background = self.image[\n","            new_box_x_min:new_box_x_max,\n","            new_box_y_min:new_box_y_max\n","        ]\n","        for box in self.boxes:\n","            if new_box.iou_with(box) > iou_threshold:\n","                return False\n","        self.image[\n","            new_box_x_min:new_box_x_max,\n","            new_box_y_min:new_box_y_max\n","        ] = np.maximum(old_background, digit)\n","        self.boxes.append(\n","            new_box\n","        ) \n","        return True\n","        \n","    def get_torch_tensor(self) -> torch.Tensor:\n","        np_image = self.image.astype('float32')\n","        np_image = np_image.reshape(\n","            (1, 1, self.image.shape[0], self.image.shape[1])\n","        )\n","        return torch.from_numpy(np_image).to(DEVICE)\n","\n","    @classmethod\n","    def get_empty_of_size(cls, size: Tuple[int, int]):\n","        return cls(\n","            image=np.zeros(size),\n","            boxes=[],\n","        )\n","\n","    def plot(self, boxes: Optional[List[MnistBox]] = None):\n","        fig, ax = plt.subplots()\n","        ax.imshow(self.image)\n","        print(self.image.shape)\n","        boxes = boxes or self.boxes\n","        for box in boxes:\n","            box.plot_on_ax(ax)\n","        plt.show()"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWMxgsgFtlze"},"source":["Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"]},{"cell_type":"code","metadata":{"id":"HezSZXw4z-cx","executionInfo":{"status":"ok","timestamp":1621284037549,"user_tz":-120,"elapsed":3517,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["from keras.datasets import mnist\n","import numpy as np\n","\n","\n","mnist_data = mnist.load_data()\n","(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n","\n","\n","def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n","    bool_digit = digit > threshold\n","    x_range = bool_digit.max(axis=0)\n","    y_range = bool_digit.max(axis=1)\n","    start_x = (x_range.cumsum() == 0).sum()\n","    end_x = (x_range[::-1].cumsum() == 0).sum()\n","    start_y = (y_range.cumsum() == 0).sum()\n","    end_y = (y_range[::-1].cumsum() == 0).sum()\n","    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n","\n","\n","TRAIN_DIGITS = [\n","    crop_insignificant_values(digit) / 255.0\n","    for digit_index, digit in enumerate(mnist_x_train[:10000])\n","]\n","TRAIN_CLASSES = mnist_y_train[:10000]\n","\n","TEST_DIGITS = [\n","    crop_insignificant_values(digit) / 255.0\n","    for digit_index, digit in enumerate(mnist_x_test[:1000])\n","]\n","TEST_CLASSES = mnist_y_test[:1000]\n","\n","\n","def get_random_canvas(\n","    digits: Optional[List[np.ndarray]] = None,\n","    classes: Optional[List[int]] = None,\n","    nb_of_digits: Optional[int] = None,\n","    ):\n","    digits = digits if digits is not None else TRAIN_DIGITS\n","    classes = classes if classes is not None else TRAIN_CLASSES\n","    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n","\n","    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n","    attempts_done = 0\n","    while attempts_done < nb_of_digits:\n","        current_digit_index = np.random.randint(len(digits))\n","        current_digit = digits[current_digit_index]\n","        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n","        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n","        if new_canvas.add_digit(\n","            digit=current_digit,\n","            x_min=random_x_min,\n","            y_min=random_y_min,\n","            class_nb=classes[current_digit_index],\n","        ):\n","            attempts_done += 1\n","    return new_canvas"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5i2OjUEC7eaC"},"source":["Let us have a look at example canvas:"]},{"cell_type":"code","metadata":{"id":"OsLpINOtvhd8","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1621284037550,"user_tz":-120,"elapsed":3505,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}},"outputId":"76a4cadf-d504-4263-882c-73521b95fb4a"},"source":["mnist_canvas = get_random_canvas()\n","mnist_canvas.plot()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(128, 128)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwcV533+8+vqnctrdZqbbblfYvjOJtDQhKyEBICYU9YQoCAZ4CBMPNwgcDcYeZ15z4PzDMvGGbYJg9LwtxAIAskkED2bUJivMR2LFveN8na91art6pz/+i2LduyY1tqSVb/3q+XXlJXV3WdKnV/u+rUqXPEGINSKn9Zk10ApdTk0hBQKs9pCCiV5zQElMpzGgJK5TkNAaXyXM5CQETeISLbRWSXiHwtV+tRSo2N5KKdgIjYwA7geqAZWAt82BizddxXppQaE0+OXvcSYJcxZg+AiDwA3AKMGgI+8ZsABTkqilIKYJDeLmNMxfHTcxUCtcDBEY+bgUtHziAiq4HVAAFCXCrX5qgoSimAZ8xD+0ebPmkVg8aYe4wxFxljLvLin6xiKJX3chUCLUD9iMd12WlKqSkmVyGwFpgvIg0i4gNuAx7L0bqUUmOQkzoBY0xaRP4GeBKwgZ8ZYxpzsS6l1NjkqmIQY8wTwBO5en2l1PjQFoNK5TkNAaXynIaAUnlOQ0CpPKchoFSe0xBQKs9pCCiV5zQElMpzGgJK5TkNAaXynIaAUnlOQ0CpPKchoFSe0xBQKs9pCCiV5zQElMpzGgJK5TkNAaXynIaAUnlOQ0CpPKchoFSe0xBQKs9pCCiV5zQElMpzGgJK5bmzDgERqReR50Vkq4g0ishd2emlIvK0iOzM/o6MX3GVUuNtLEcCaeB/GGOWAKuAz4vIEuBrwLPGmPnAs9nHSqkp6qxDwBjTaozZkP17ENgG1AK3APdlZ7sPeM9YC6mUyp1xGZBURGYDFwBrgCpjTGv2qTag6iTLrAZWAwQIjUcxlFJnYcwVgyJSCDwMfMkYMzDyOWOMAcxoyxlj7jHGXGSMuciLf6zFUEqdpTEdCYiIl0wA3G+MeSQ7uV1Eqo0xrSJSDXSMtZBKvZmo2YWf+BkvlyBAoczLQYnOHWcdAiIiwE+BbcaY74x46jHgDuBb2d+PjqmESp0GP3FuOYvTykeJ5aA055axHAlcDtwOvCEiG7PTvk7mw/8bEbkT2A98aGxFVErl0lmHgDHmvwE5ydPXnu3rKqUmlrYYVCrPjcslQqWmom308zuacYFVlHEtMya7SFOSHgmoacnF8AgHWc08vspiNtBLG8OTXawpSUNATUsHGKIcP2X48WBxARG20D/ZxZqSNATUtNRPihJ8Rx6X4KWf1CSWaOrSEFAqz2nFoJqWwnjpI3nkcR8pwnhPnFEEe+E8jD/zUbA6ekm3tWee8niwa6tJV0eIzgyRLMhcES9f34vVPUC65VDuN2QCaAioaameAjpJ0E2CMF5ep5fbmX3sTCLg8dB+dQXJcOYDXrmuAE9HF7gOVmEB/RfX0LZK+Oj1L3NbeC0AH/zxlynfXIRfQ0CpqeszNLKEJF+iEQf4PPANth07k4GCFKz+8f8Y/UV6gQezPyNs5e8AaCPE7XLTOJd84mkIqGkpTJLtrOSzI6Z9d+QMItBQywOFQ1Q+0smNoUEAFjy1moU/SCDb92OFi9n5uZlUrmznwSW/IGz58IoNwPWN7+el6/91wrYnl7RiUOUdsT1YBSE6V/qYdXELF/s7SJgUe9NxGLaxhlPgOLj9A9Q9lyT+cBVvffDL3NO/YLKLnhMaAir/WILYNqbQYYG/jSLLQ8w4vJGoxjNoI0PDGMfBJJMEWgYo3pskvF3YM1wx2SXPCT0dUPknWyHoDyap9Bj84mV7yuKeg1dStA/S+w4cmdXZtpNARymBQ+Xs+nAFVE9esXNFjwRUfpJjb4AddAPs747gjZ44qxmOI70D7Git5OFoOQkzvRod6ZGAmhYSBI7pIKSAU3UYkkZcGOz08IULMhV9fW6IRFcQ35B7wtxuPAFuH7Q08NisFVwTas7BFkweDQE1LRzfRdhq08SDsuzEGS0b9y3L2fVRD59Z9RIfKitiWyrFI10rmfkEFOzowjl+GdfBTbjYceiMF5I0o3abec7S0wGVP0Sww8UMVfu5dOluLgrtwTGG54YWsaG1noI9fdDTd5JlLbDAaznYE1vqnNMjAZU3rGCQ4Uvn0XGh8MdZjxMQD51Okh8/fCPlm13cHesx6fQJy4nfj1VUSLI6xXUVTQRken13Tq+tUeoUxOMhWuPBKUsREh8xN8Uhx0+wXQi2JTDOCScCR5aTYJBQeJiVwX34ZXp9d06vrVHqVHxeBmdDeWVmeIxO19CUqKbwkIPvUB/pk5zri8+HKQwyq7SXywMpmGYnBHokoPKG+P1YSwa5umYnAFuTVbzcv5BQSwy3vfPkC9ZU0nlZOXMKuyeopBNLQ0DlBSsQwIQLWVLVxoqCA7i4bBmuY0NHLXbvEO7Q0KjLicdDsqKAgTlQH+gBwMXFNSfraPvco6cDKi9Ebzyf7mU2P679DbM9UfakLO7709to+G0M07Jz1GXE78eaO4vWywJ8+j1PcnPhG7h4OZROEE34Rl3mXKRHAiov9M21SS2OUWtHCYgQMx58vYJ3XzsmmRx1GSsUovf8UmKzU7wltJMiy6XdSfCt9uvp2xuZ4C3IHT0SUNOfCOlLB/nBBQ9QbfuImhSDbgDfIKRb206+XHmE3vcO8YF5W7jEb2h3YENiBq/dfwGzGxMTV/4cG3MIiIgNrANajDE3i0gD8ABQBqwHbjfGjB61SuWYVVCAFSmhMJigxM40I96QKOX/2fVOQu2jXxJEhPjNF9O91MMnFz/NFQXbAXg0upjHWs+nYlMc/95TVCSeY8bjdOAuOKbLlm8D3zXGzCPTN8ud47AOpc6cCFZJmFR9OSXBYQokjS3C1ngt7VsqCXaOciOQZWP5/XSs9CCX9rG6ZBOX+A0uLi/2LGDH7mr8TS2k9x+c+O3JkTGFgIjUAe8EfpJ9LMA1wEPZWe4D3jOWdSh1tjwNs2j+4GwKv32I/7vh98zyCHGT5vG2ZSz4WQ++TXtPWMZavpDe96/gLTdu5t4V9xKyvHQ5w6xJeNn2u4Us/t4ATtf0ulQ41iOBfwO+Ahy+9aoM6DPGHG572QzUjragiKwWkXUisi7F9Dm/UlOHU1pIbIbhzpqXme/J3CP84nAZ+9vLMPtbcAcHj8wrXh+eGVX0LyqmayVcH2lkmU9wjGFDspwftl5DyR4Hd9e+UZsWn8vOuk5ARG4GOowx60Xk6jNd3hhzD3APQLGUTq/bstSUMNhQgFsb5+3BISDI3nScu/77w5Ss9Z/QLsCuLKfzulkk3tvHny/8CWErcwmwy03yj03vpuCHYYo3HSSdmH5fWGOpGLwceLeI3AQEgGLge0CJiHiyRwN1QMvYi6nUmRuYbVNWerSXEMcIxG3sZPY7RzLdjNl1NQwtqaL7+ji3zmokbGXuK9iaCvDp9Z/FXldExa523IHBk6zp3HbWpwPGmLuNMXXGmNnAbcBzxpiPAs8DH8jOdgfw6JhLqdRZiDakWRA5WoufwsIatrCz16rE48UKhYjPraDrPC8/WPVL7iz9MxYWfa7Li9HFVNwfZObvu3B27D7m9GE6yUU7ga8CD4jIPwOvAz/NwTqUOinx+yEO8xa2ckVJpjVgqzPMa8MLmPlkGn93HPey8zl0eQHR+SmuPr+J9xXvY6W/h4NpH2uj5Xz9mQ8R3mZTs3Yfbt/0Hsh0XELAGPMC8EL27z3AJePxukqdDaskDG2wItLMHF8HAE3JCBujM7FShlSxn775PmLnD3Pdgh18fcaTlFgWbY7wcmwBz3QupvR1i9LGIdKt7eCepD3BNKEtBtX0YtlEV82G38EXyl+myvYDFl9tfD/9B8KE7+pnZVUzX6/+E0WW4EXoceGJWD3/c8s78D0fpubxFio6NmMSCcw0DwDQEFDTjFjCwMzM27pILKxstdcl1fvZHqziqqqdLA8eZKYnSMo49LtJvtH8bja21OJ/tYiyxjhueyfu8DBMs74ET0ZDQE0vtk3/4sx1fO+IbsC+X/vfJ7RYiZoUO9OFND66iBlvpPD/6VUwhhP7G57eNATU9OI4lG3I9PyTMi7+UW77P5Ae5oXYPH6w8yoGdkaY8/IQ3oNdJ+1ZaLrTEFDTinEcSnZnGvT8OVFKmXViZyGbEwv4XdsKhjeUUbMxjadxL+mBgYku6pShIaCmF2PwvroVgB+8610njDQEgOMiyRQN0Z2YWAwndrJBSvKDhoCadtx4nDZC/Gnr/8rpetoI5fT1J4qGgJqWbpebJrsI5wztXkypPKchoFSe0xBQKs9pCCiV5zQElMpzGgJK5TkNAaXynIaAUnlOGwspNYqo2YWf+BkvlyBAoczLQYlyR0NAqVH4iXPLWTQLfpRz7z4EPR1QKs9pCCiV5zQElMpzWieg1GlK4fJ9dpDG4GI4nxLeQc1kF2vMNASUOk0ehM8xHz82Dob/YDuLCDObgsku2pjo6YBSp0kQ/GT6L3QwOBhG6bfonKNHAkqdARfDd2iiiwSXU8Gsc/woAMZ4JCAiJSLykIg0icg2EblMREpF5GkR2Zn9HRmvwio12SyEL7OYb7KMAwzRyvBkF2nMxno68D3gT8aYRcD5wDbga8Czxpj5wLPZx0pNK0E8zKOIJs6gl2IRsGywbMTjOfrj9SFeH1YgcMIPlj16Z6nj6KxPB0QkDFwJfALAGJMEkiJyC3B1drb7yIxR+NWxFFKpqSBKChshiIckLjsY4BpmnHohy8auKINwEanKImLVfhIlFoOzwM1++tIlDp7iJBfP2k+5/+hQ6q6x+O9fXErluiGsddswqWROtmssdQINQCfwcxE5H1gP3AVUGWNas/O0AVWjLSwiq4HVAIFp0murmr4+xRb2k+QOwAFcMt9+/8DuY+YrAFabpqMTHDKfgjZg+9ms+f4jf7URykkHqmMJAQ+wEviCMWaNiHyP4w79jTFGREYd1sUYcw9wD0CxlObn0C/qnBEmybOs5OPHTf9u9rcVDCChEL8udfnee24mujDFpUt2s6zoEDN9Xcz09rDEN5iZl2OHSLMRLCz8cuzH0cXQmExzf+8qtr6zij8e+n5Otm0sIdAMNBtj1mQfP0QmBNpFpNoY0yoi1UDHWAup1FQmtofBC2tIlAgDphPfVV1cXdHKDZEt1Hu7KbXilFguZVbmiNflxO8867iLjYfn+e3ASp46sIjadFfOyn/WIWCMaRORgyKy0BizHbgW2Jr9uQP4Vvb3o+NSUqWmKPF5KL2sk2tLtlLdnuTOlfuPm8N/5EM+WgAcnn5iELg80HQh3g2FmHhzLooOjL2dwBeA+0XEB+wBPknmaOc3InInsB/40BjXodSUI7YHCQXpv7CS+NwkHwmvodo+tuKuOT1Mv+ulzSnmjXgdr/bM4eBAhKG4D4Bk0oPb7WfWojbeV7ORW4u3UmYFAWhMpnkhtpDAnwuZ8eogZjh3lyLHFALGmI3ARaM8de1YXlepKc8SxGMzXGW4fmYTMz3DBCybITfBrlSCHjdAY2IBHalidscqaOqtpG1/Gb4uG8+Q4AQN3rTgG4TmsggHykqJZ0dFztQF1PB423mU7Elj72zGcZycbYq2GFRqFAkCx3QQUsBxHYY4FhIbpqdLaG9JstHvMOi6PMoqfvTwjZRtEkLtKbwDKbyHeihNDhFJD0A6jQT8HHrvHPqWp/nPt9/DDHuICksotIK4GPrdOP+y7e2Ef15E0YZm0n19kMNh0zUElBrF8V2ErTZNPCjLRkwRLMuPPfcCnl2V4vXKAYYTPsyWYso2GiLbBrG7BjBDw6S7urDLSnFnzSBWX0CszKZvRYo5c9pZ6YvjFR8WFo/HwmyKzeTx5qWk1kYo3NWNOzCY0wAADQGlzo4xuPE4Zf/nVcr+zyhPA+kRj9ML69n3riBvv3YDX6x8jlkeHx5sIFM/MGySfH3Te2BTMTP/ZT2RxE5ydwJwLA0BpcaDZSO2jVUQhJoqEtVFDFd4SRQLA/NA6mO8a8EabolsoMLKtAtwMbi4/HvvIn7XfD7Bp4so2ZnApNJvvr5xpCGg1FhYNuL1YIVCSMCPiRQTnV9C31wPQ3UuVMT5/AUvclFoD5f7XQBc/EcWTxmHZzsW0bmhijnrBrD2t+FagjGS89OAwzQElBoD9/Ll9CwJ0HdFnNrKPm6tf5USO0aJPUSJFSMgaeo9KUJy9ND/sJRx6HHTBOw06SKXgXlFBCIBxAV/cx/Ojt2jr3ScaQgodYasQAArUkJqdhUdFwYZWJTm7QubuKx4Nx8uasE65uZcG7BPaAh05LWA+UUd7KkvpXt5CZ6oHzEQCZdT1D+IGYxikklMOnenCBoCSp0Jy0bqa+i6rIqKT+3jb2es56aCvYQtX/bDf/p353vFpsoO8s9Vf+GfKtcQuzBF3Bj6XZvbXr8TYzdQvK0P6ejB6eriJI0Nx75JuXlZpaYnsW3SFUXEqoQbKrZynr+ZQvHiyX7bn+wb/1QsLLxiU2T5KLV81NiGj81bS++Ho+y6vZTWW+djhXJ3p62GgFJnQGyLRJmfeLnhpsJG5njSJ9z9N5KLS8o4JEyaNJnfKeOc8OPi4sHGLx6KrQBfLt3OxlW/4DM3P4X3xk4khyGgpwNKnQnbJlZhky5JE7bkSADY2VuDHePS78bpdITHBs/njcFaXmmcD87JjxDs4iThohhfX/gnVvgPMduT+cBbCNcUbKNlRoSdnvKcbZKGgFJnwhjshMEe8PBIdD4BSeITB0uyl/+MRVs6TGuyhKcOLKKvs5DIeg/WKer1EpEQg5EgL1YvJFCSZKZn6MhpRYmVpNwbZadVmbNN0hBQ6gy4sRglD71O2QtlPPiHGzCWYI7rA9BOOFiJNDUtXcyItkAqhTnFNX+xbcTv5wnvRWw4r56rlv2SoPhwMbQ5IQ4lSsB1c7ZNGgJKnSGTSOB29+CzLLBHqVZLO5hUCqen77T6BTSApNIEO4TWrjBOdoQjgAJJUeQ58yHSz4SGgJqSomYXfs78zZ8gcMLNP7ngxuO4B8exow/jUnTQJV4WwDWGwxcZyu0UVd4Btli5G+5MQ0BNSX7i3HIWHdAec7vvOUL8fqySMN3vjfGRReuPudqwLjGD1/oaIIf9CeglQqUmiXg82JEIds0M3JlVfHDh63ypdC1esUmYFO3OMK8MzmdrxwxtMajUdCRL57Pj9jChef1cUbuTT0ZepdgK4WK4b2A+P9l5OZ5HI9RuieL29eesHBoCSp0m8fux/H6kLIIJBUhFgnj64ljRGG53b6aNfyJxkoUF8XixwkVIQYj43Ep6FvmpW36Iq6p2cmVhE+W2TcKk2ZV2ebJzKbE3IszcGcfe367diykF0EGcX7D3yONuEryDGq4id9fQR7JLI7hlJXRcGiFWI6SXRrGbSinaG6FsfQi7q5d0W/uoy4rPh1VYQGrpTAZmBpAPd/LOmu18s2LjkTYBtgRpTkf5WffVbFvTwPz7uzAHDuEMDeV0uzQE1DmjkgBfZjGQ6Yzzn3iD8whP2Ppjy+voXeAjcGMHK0u6WFl8gDdm17JvsJTtbyvHxIrxDMzB3ysEugy+qMFKZy71DVXb9C908FXFqI4c4mN1a1jkP3QkANI4fLtrMU+3L6L397XUbU9CR/fJjyzGkYaAOiftZJAy/JSO6KAjZ0TAQN9cH/3LU/xm6X00eAKZ5yI7cXHZMs/Q7RSwL1XBnzqXsvlgHW63DzueqXsPzO/jP857hPN9XVTZwSMvncYlbtIMug4P7VvB0BulzL13C24shpPDysCRNATUOel1ermAiRn13lq+CDZC6fua+fuZL1Bj28c+j8Vir0PKO8AKfx9Xh3bSUxdgyPhwTCYEKu0oDV6XgBwNrXZnmOdis/n+7rfRs62Mht8nqN7fQjo6BO5E9TCoIaDOQWlcGunjneSuAc1Ixsp8kGcW9rLCfwivHHv0YZG5kciPh0KBiGVo8AAkR8zjBSBh0sRJ0u64bEzU8l8tl9GztZyyzeBtaiHdPvGj9mkIqHNOEwPUEqIo+8HKNas588Fs7J5BX5WPamOwzrzbAADanSQHnUJ+3vFWXto1j/pfeViwswNn1z6cCfz2H2lMISAifwt8mkzz5zfIDENWDTwAlJEZrvx2Y0xuBlZXeWkDvayk9PRmFgGxsOc3kC4rYKguSCIsxCoF128QR5hzzx7SrW0nfQmTrZ1PPVHBrU1fxAm6nEXfIQBYCQsrKfi7hbJ2Q2hXJ3T3Tujh//HOOgREpBb4IrDEGDMsIr8BbgNuAr5rjHlARH4M3An8aFxKO06mert0daJPsYUwSYaAfwZeoJcwxw/8mRkpaLVpOjrh8M17TSfMeow2QtwuN436nBuL0UaITd//u7Mp+rhpO4tm1KdjrKcDHiAoIikgBLQC1wAfyT5/H/CPTLEQyKd26dNFmCTflYsQr4d/8Hm5t7AA47WhtRPjuFgBH4jF74jx8MKbSUUC9M3zE62H1Jw4l8/dzYqiZhYGDlFhDzLfkyJkedmfTvLF9/8VT639x1Ou/2QBMR2MZWjyFhH5V+AAMAw8Rebwv88Yc/jaRjNQO9ryIrIaWA0QyFHCqWlCLDBgR8KYgI90cYBUgYWxhVC6FHENboEfYwtOykPP8jCJEmFgoUPZrF4+NHs97yvaxEzP4UtzFiOHC893YzkdiAC3AA1AH/Ag8I7TXd4Ycw9wD0CxlE7MKAvqnGQVFcAAtF0ToaA6yseqX2DQ9dPvhnixcwE+2+WCkh2UegYZbB3gfV/ZiVdcQmIIiBAQ+4QafXXUWE4HrgP2GmM6AUTkEeByoEREPNmjgTqgZezFVPlMfJlBOwqqo5wXOUSNx2LYHWbIDDNQehCfOCz0t1EkhgrbYoHXd+RbPmoStDtp2hwf3U4hW4brqfN1c31oH2HLd6rV5o2xhMABYJWIhMicDlwLrAOeBz5A5grBHcCjYy3kRNhGP7+jGRdYRRnXMmOyi6Sy3EDmUuAHZqyn1k4CHkKWlxBwfUE7BkNmkA84dhhQOJQ2rI3P5uX+hewaKOfAlmr89VFmXvBLFnoHOOtq/mlkLHUCa0TkIWADmT3/OpnD+8eBB0Tkn7PTfjoeBc0lF8MjHOSvmU8YL99lO0sJM4Pgmy+sciJB4GhFbLKfEPDw9jrmBjtY5o8iIz68JnsJIGkcpEL4+44L2dRby47GOvxdNqE2g28g01uPd6GFiGGhd4Cw5WPQSU3C1k0tY7o6YIz5JvDN4ybvAS4Zy+tOtAMMUY6fsmw79AuIsIV+DYFJNPJSrF25kNUdr/C/bvgABbMO8sm6JyiyUgTEEDdCyljEjU23U8ChdIQ/NC4lfqCImU85BFv6YfdB3OE4dmkJfQsWEvClKLeD2VMGDQFtMQj0k6JkxGCRJXjZr5cDpwx3R+b24UV3dxLzF/H3gY/T/tYIQ3VQuB+CPS5FO/oRx4AxzI4NQqIbd2AQk0xh0ikwBikq5IZ3/YX3RdZhIdhiYedqbK9ziIaAmvIO99ibHtGxZ3l4BYGeAIUHhvF0R990BF+roACnpJDzQs3M9kSBEFE3Trfrn7AhwKcqDQEgjJe+ETd79JEiPEHt0tXZkVc2UiQCxnA6DW5lZg2Dcwqp8AwQEsHFsCcNa4fnIOnc9el/LtAQAOopoJME3SQI4+V1ermd2UdnEMkMEOELYdfNoffiSoaqLaKzHYwn8y0SaPUQ6IaS3SkSYZu2Kw1FO23KtibxdQ9jbIveJYXYSYNvwCXYOoTdO4RzoDmnnUhOa6fxDW4XFyNlEfa+v4L0sijzvV0EJHMl4cedV/Nk02IWDfTkuqRTmoaAWHzGbGYJSb5EIw7weeAbbDs6jwHSEErD6h0bYMdpvO4Do0xbM/qsp2q3rsZGigpJ1kaovPIQ35r3MLM8HrzZEHhh/zyK/hLERPO7/ievQ8AuK2V4Zpjw6+vYzko+O+K574742yosILakil9Feuj9jpeVwX3MsKM0eGy8khmSusWJcTAd4v/dfzOOa7GqfC/NwxHa40XMK+qk2tfPZQU7ibteBt0gPz90OVv3V7P4qy38sfUHE73pecOtKKFnSZCryzaz2JfEK0crgBPDXor6DOT5kVheh0DmNtNTz2KHi3FKi4jWQmlokC9FurLP+IiZJDE3RdwY1sZr2DJcR0t/mHjCy76uUrxeB58nzVtK97AseJBL/SksHCDOa8Vt7Ckog+N6qVHjxLKxS0uIziqkfz7MD7YTEh8p49DlJmlKRqDLT7A7nfenY3kdAk53D/7e3pM+L7ZF74UVDNel+fKSp6hqMUA5FkIah70piwPpMtbHGnjkv66i/rF2ajsOYZLZa8/ZuoRfrb6Wn54/zIa3/ZCwFSRlHB7dvpzgXwowQwcnZmPzjF1aQveNC+i8PsHvr/wedR5w8bA/neYn3Vfy6HOX0vBEAvvFTbiTeC//VJDXIYAxGGf0yiWZWcPwjADJOUnmlXVTKB68crQWudNJ8NmmO+joLkZaA9RvSUFbJ87g4LEVViJUbEzQKUFiVzkUHh7C2rWQ/P4CyhkrFILSEqJ1QnnZIHUeCGSH9koYm+2DVZS9Ab62wUnrzWcqye8QOIXBhgAsG+K26o3UeYawxYvB4OLiAvvTIZK/q2TW7iTBpgO4Pb04sVEqmIzB+8x6aqLLGfwboTz7Gip3rHAxqYoiYrPSXBDppDB7B2EahwHjZ293KbNfasHtPvlRYD7JyxA4pl06md5oDj+W8lL6lgQJVR/kCncXT7cn8GFhi4On0rArlebm575A8SYftc+3Q+8AzsAAJnXi17p4PFihEC2fXMbA8iQVlmSbquoQkDkhgvh8tN7SQN9iw91XPcaFgX0cfpvvTye545nPEXndg9u2GzepTYYhT0Pg+C7CVpsmHrTOw66sYOji2fg+383ts1/lzuLMwBYuhoRJsSXp5Q+Di4is9TLjpW7cPftHr1TKDjlll0UwZSUMXpDgusVN+MWDiyFmkjhxG0/cgJvfrdXGkxUKYZWE6V9kaFh6iI8V7Tuyz39oSMYAABJFSURBVLucYTYmagm/4SWyPYEbP/Pu5aarvAyB0dhFRWz/2hxmLO7g+wsfoN6TAoK4GHrdOE8MNfBPf343837uMGPHbtzunpPWKluFhTC3ngPXlWDe0s+/nfcr3hLoxCsBdqSSvDI8l8g6L1XPHMr0Ma/GRXLVItov8vOl6x/n1uKt+CVIGoeYm+Izez5IY1M9Sx4/hNveqSdkI2gIHOb1UDKvh3fWNFLvSVEoXlwMryZs1sSW88O1V1Oyzodvzz6cvv4TAsAqKEAKC0gsrWe40kvvQguzdJBbGhpZ5OukyMpcnmpKVvFY+/kEu1xM3wAYfTuOlRUKITVVtC/1k1wxxMrgXsqszB2gh9IJNiVnsLVxJqWbLExvP+6wHgWMpCGQJT4fd81/jo8WdeASOHIZ8J62a3mlcT6L7mrEjcU4WYW+VVlOcmYp+z/tcNWc7fyw/nk8Rzq6yHwj9btJnupbyvY1s5m7dxDnFJcn1emzykrpvGIG7jW9/HbFz5jlEcjeFbo2UctPD76V2b9N431m/WndZ5BvNASyBlbNoszzwpHHP+6fxSOHLqDr8Tpm7kxjkscOnSB+P+5Fixmu8tPfYDM4P02kpp+75r7GkkDmbjcXg0XmZpUtScOtf/4C/sYgs18Zxtrfrm/IsbJs7Lmz6F9eQfTmQW5t2ESV7eIVH61OjJ/2XsL92y6i4OVCqve16v4+CQ2BrIFZNgWSxM3eX76mv4G926uZ99oQnj2tmGDwSONC8XmRokI6lwaJzoLi87v4qzmv8r7CbZTamctRjjEMmjgxYxhyLV4cWkb4xQBljTHklY36hhwHYtvEG0rpXWDz+SUvcnlwF5HsaUCP6+GxA8vwbiqk+pkOTOvED+91rtAQyEoWQ0BSR76575rxDFdcs4tvF9+A6ZnDkXZCApdevJ2rS9dR632KIivODHuIUguKLD+DbpJ+17A7FeHube9l+LVyqtYlCbRGqTq0AxMb1kqpcWIVF9L9uSE+1PBnbi1qIiRHb//eGK8jeG+Eyl29uHsOZDoWUaPSEMjy9UPceDncUWW9J4U3uIfL5u6ltbr4mHn/uvp5Lve7uNmGP3FjsT8ttCVCHEzVcyBZxovt84luLmPG5jTB9XtxuronYaumKRHs+XMYnlPKVXUbua5oCxErcyVn2CT5w1A197esomj3INLWjZvSUfBORUMgq/qlfto+F8Ylc4NQxAoQseCemU8BYIvgZJsDe8U+8m3e7ybZl/bxrYM3sflgHabDT6jFYuYjh5jb24TT13dkOTUOLBsr4GfPxyqZfcUBPl3+MrM8BhcfUTfBIUf4h0duo3yjobhxgwbAadAQyLIGY7Slw/S6zUSswJF+6w/fe24heOTYyr52Z5hHo4v5jy1XY20qonyfiy/q4u9LYLp6MpeiNADGjV1WSmrZLA6+NUjtW5r5SM0aZtgO3mwvUP/ecxG/2XUBletdipr6cfUU4LRoCBw2OMSe4QoOhrx4PQksGf0eY9cYLBFSxmVPupDfty2n6I+FVKzpwtl6tLeR6VDxd7YDt0KOBm+NhOlcHuTWD77Ae8MbWOr1cbhBV8o4PLz3fIJ/LCb8yt5TjjKsjqUhkOV297Dtb8/jy6HzMZ7TGJDCgJU2eAeSVB7cj9vbl/tCTrCzHbgVcjN4qwn5SZTCx0v+QoWdeeumcfjDUBlfWfN+Sp8PUPn0QZyu/O4u7ExpCJDp3uup1APw0mh9gk3M+tWbk7SLNwqPRpcdmZYyNs+0L6ZgY5CSHTHSzS16CnaGNASY3sNOTyfOtp3U7jnAU/fOOWa67QxSG9+QuStQA+CMvWkIiMjPgJuBDmPMsuy0UuDXwGxgH/AhY0yviAjwPeAmIAZ8whizITdFV/mkjRBPuw9mRr0czs3r56vTORK4F/g+8IsR074GPGuM+ZaIfC37+KvAjcD87M+lwI+yv9U00UuSX7KPaLY9xWWUcyWVOV+vHq3lzpuGgDHmJRGZfdzkW4Crs3/fB7xAJgRuAX5hjDHAayJSIiLVxpjW8Sqwmlw2wi3UUUeIOA7fpYkFFOm4jeews+3ipmrEB7sNqMr+XQuM7DmzOTvtBCKyWkTWici6FImzLIaaaMV4qcseOgewqSRAvw7qeU4bcz9X2W/9M66NMcbcY4y5yBhzkTc7GrA6t/SQoIUYsyiY7KKoMTjbEGgXkWqA7O/Dt2i1APUj5qvLTlPTTAKHe9nDe6gjgI6dcC472xB4DLgj+/cdwKMjpn9cMlYB/VofMP04GO5lDyspZTmRyS6OGqPTuUT4KzKVgOUi0gx8E/gW8BsRuRPYD3woO/sTZC4P7iJzifCTOSizmkQGw6/ZTyUBrj5SFQSIYFVVkA4HiNZ56XV76P/kyqMjPBmQHh8FzRY1Lw0iTftwo1G9rj8FiJkC/4RiKTWXyrWTXQx1nJTZckyz4U+xhTdI8lbgPI4eRv5PMsk/0j3A6hyUSQdvPXvPmIfWG2MuOn66thhUpy1MkrWs5DvHTW+aX8/6ai/1b2lhaUEL1Z4BOlvSNH2liApL8IsHv3iImgSH0obPNH2MtsZKFvzLHpz2M+vx52nz0PhtkAI0BNRYiIV4PQxVeBmqd7mwcB/VniG6nAA7EqX8Q/N1lPuGCNpJwp5hlgWbeVuwk+urm3jcsXFrKrCTKe1wdZJpCKizZgV8EAmTOC/B/9XwIjFjsSVRxZP7lrJ10yDlzxUz4C3FeD2kSwL84p1BfnLrj7gz8hduLt7IJ679EuVbQvj+tHayNyWv6XhYaszSSZs9qRJ+tOsqnt50PkXbPITaU5iuHujoRto68e3vIrINvrD5w6yJ11BjJ/Fe2U37xd43X4HKKT0SUGfNGBBjIOZh81Adpa9a+A714PT1IcRw5Nh7Cko3FdFWFOHFuoW8u6CX/1p+Lx+XO0BErxJMIg0BdVKnGrg1M8MwdA3C88W88poPu2MfJpEEHBIEOP473uropXxzgH1DZVgINbahtniA9IK50NGtdQOTRENAndSoA7fKsmNnSgOdhx+UZ34JJwQAgIkn8PbEGE5nng1ZXkKeJH0hP5ZH34qTResE1KRxjCGe9mL3RSGhN5FNFo1fNamSrg3R2AnDvKmJoyGgJo2LS8Lx4Olq1YrBSaSnA0rlOQ0BpfKchoBSeU5DQE2cshIGFpdQFRzExbA1ZdMV1V6JJpuGgJowQ4sr6PzgMDeXbSJmkvy080oGDxa/+YIqp/TqgMo5KxQifeFCOlZ6+Miidcz3tbM/LTz3/AoqNxu9MjDJNARUzkkoSNfyIM6iIT5fupaYMWxIzKDmZYeCps5pMXjruUxDQOWUeDyYGRXM//B2bqnYSNgK8Lm9b2dt4xyWNLbjHNLRgyeb1gmonLJm1xOdH+aWio2s9B8kZRy2tFVTuNuLGRjEaHPhSadHAip3LJu9H6smcEEPN4QOYCM0OynYVEzdU324A9HJLqFCQ0DliPj9WIUFJOcO8/5ZjYTEy88H5vLd16+jutHBau8h7WhtwFSgIaBywgqFoCzCxXP289elr+KVIP/f/kuY830Xz649pDs73/xF1ITQEFDjyo5EiF84h31v87H4ij38Xc2TtDs+rn750xT8JUhk53bc/sHJLqYaQSsG1fixbCQSpvs8P6UrO/j5nN9SZiVYH59NaH2Qsm1JnO4eTEpvG55K9EhAjQ8RPLXVdF82g+/8zX+y0NtPyPJzw+aP0PtGOQt+tQe3tw9XGwZNOaczDNnPgJuBDmPMsuy0/w28C0gCu4FPGmP6ss/dDdwJOMAXjTFP5qjsaiqwbDzVVbhlxbS+JULfeS7n+waIGdiUhJ5tZZRvMbj9A7inuBwoXh/i8yIBf+aIwuclNbOC6MwgdsrgGXLwP7c5845T4+p0jgTuBb4P/GLEtKeBu40xaRH5NnA38FURWQLcBiwFaoBnRGSBMUargacjESyfl+GlNXQv8XHrp57l5uJNRKwgW+Ieft+3grpnHYIvNuLGYqd8KStchBQX4UQKcP0e0oVeDl7n5X3Xv8r+WCmbDtXSsK4QuiZo2/LIm4aAMeYlEZl93LSnRjx8DfhA9u9bgAeMMQlgr4jsAi4BXh2X0qpJYxUVwQAk33Ex8UhmKPJUgTBcKTgrBnnrrG3cXLyJetsFYF+qnC19Nex/L1jXL3/T13fDaXyFSSJFMQKeIfx2mo+U7+Xdxa/TVxzkt76L2OMJ53Qb89V41Al8Cvh19u9aMqFwWHN22glEZDXZMSsDIwa9VFOThIIwAF3LvAxXZz7oTnGa2vpu/qbheT5Y2A34jsw/5PqJpny8fcUWKnxRQtapj+MXBlqZ6+1kntfgFy8p45DCwTWGFANsDbWx19Jh0HNhTCEgIt8g0+n0/We6rDHmHjKD11IspVpbNMU5DTOgDS75wGb+qup5AALiUCRpSm0b8B8z/0eKtnPT4m0EBLwiJ7yeCySNIWGgz/UREAevuHQ6hh7X4rGBC2gcqGZPbxm9XUX4Wr3MHWqcgC3NP2cdAiLyCTIVhteao+ObtwD1I2ary05T5zhrOAXAy/vmMOT43mTu05N2LZKu58g4BIdFkz7a20uQPi++PotwNwR6XO2ROEfOKgRE5B3AV4CrjDEja3weA34pIt8hUzE4H/jLmEupJp27aRtthNh16z9Majna9NRx3J3OJcJfAVcD5SLSDHyTzNUAP/C0ZA71XjPG/LUxplFEfgNsJXOa8Hm9MjB93C43TXYRVA6ImQKNN4ql1Fwq1052MZSa1p4xD603xlx0/HRtNqxUntMQUCrPaQgolec0BJTKcxoCSuU5DQGl8pyGgFJ5bkq0ExCRTmCIqXGjaDlajpG0HMc6l8sxyxhTcfzEKRECACKybrSGDFoOLYeWI7fl0NMBpfKchoBSeW4qhcA9k12ALC3HsbQcx5p25ZgydQJKqckxlY4ElFKTQENAqTw3JUJARN4hIttFZJeIfG2C1lkvIs+LyFYRaRSRu7LTS0XkaRHZmf09Ib1biogtIq+LyB+yjxtEZE12n/xaRManT69Tl6FERB4SkSYR2SYil03G/hCRv83+T7aIyK9EJDBR+0NEfiYiHSKyZcS0UfeBZPx7tkybRWRljsvxv7P/m80i8lsRKRnx3N3ZcmwXkRvOaGXGmEn9AWwyA5jMIdNd7SZgyQSstxpYmf27CNgBLAH+BfhadvrXgG9P0H74O+CXwB+yj38D3Jb9+8fAZyegDPcBn87+7QNKJnp/kOmdei8QHLEfPjFR+wO4ElgJbBkxbdR9ANwE/BEQYBWwJsfleDvgyf797RHlWJL93PiBhuznyT7tdeX6jXUaG3sZ8OSIx3eTGdhkosvxKHA9sB2ozk6rBrZPwLrrgGeBa4A/ZN9UXSP+4cfsoxyVIZz98Mlx0yd0f2RD4CBQSqb7uz8AN0zk/gBmH/fhG3UfAP8JfHi0+XJRjuOeey9wf/bvYz4zwJPAZae7nqlwOnD4n37YSccqyJXs4CoXAGuAKmNMa/apNqBqAorwb2Q6bnWzj8uAPmNMOvt4IvZJA9AJ/Dx7WvITESlggveHMaYF+FfgANAK9APrmfj9MdLJ9sFkvnc/ReYoZMzlmAohMKlEpBB4GPiSMWZg5HMmE6s5vYYqIofHeVyfy/WcBg+Zw88fGWMuIHMvxzH1MxO0PyJkRrJqINNjdQHwjlyu80xMxD54M2MZ72M0UyEEJm2sAhHxkgmA+40xj2Qnt4tIdfb5aqAjx8W4HHi3iOwDHiBzSvA9oEREDvcGPRH7pBloNsasyT5+iEwoTPT+uA7Ya4zpNMakgEfI7KOJ3h8jnWwfTPh7d8R4Hx/NBtKYyzEVQmAtMD9b++sjM6DpY7leqWT6Sv8psM0Y850RTz0G3JH9+w4ydQU5Y4y52xhTZ4yZTWbbnzPGfBR4nqNjPE5EOdqAgyKyMDvpWjJdx0/o/iBzGrBKRELZ/9Hhckzo/jjOyfbBY8DHs1cJVgH9I04bxt2I8T7ebU4c7+M2EfGLSANnOt5HLit5zqAC5CYytfO7gW9M0DqvIHNYtxnYmP25icz5+LPATuAZoHQC98PVHL06MCf7j9wFPAj4J2D9K4B12X3yOyAyGfsD+CegCdgC/BeZWu8J2R/Ar8jURaTIHB3debJ9QKYC9wfZ9+0bwEU5LscuMuf+h9+vPx4x/zey5dgO3Hgm69Jmw0rlualwOqCUmkQaAkrlOQ0BpfKchoBSeU5DQKk8pyGgVJ7TEFAqz/3/ZZIKGKcUzEwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4QkpHGiOCnng"},"source":["Now - let us generate test canvas:"]},{"cell_type":"code","metadata":{"id":"PV410KSYC-eT","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1621284038034,"user_tz":-120,"elapsed":3976,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}},"outputId":"9cb4152b-f1aa-41f7-bfb0-9b7c50e99761"},"source":["TEST_CANVAS_SIZE = 256\n","TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n","\n","np.random.seed(TEST_SEED)\n","\n","TEST_CANVAS = [\n","    get_random_canvas(\n","        digits=TEST_DIGITS,\n","        classes=TEST_CLASSES,\n","    )\n","    for _ in range(TEST_CANVAS_SIZE)\n","]\n","\n","TEST_CANVAS[0].plot()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(128, 128)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"OgU2bKyJ3pVh"},"source":["For training one can either:\n","- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n","- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."]},{"cell_type":"markdown","metadata":{"id":"Ri3Jh7GzjZ4W"},"source":["### Anchor size analysis (0.5pts)\n","\n","For this task:\n","1. @DONE Sample at least 1000 random canvas.\n","2. @DONE Analyze the sizes (heights and widths) of the `MnistBox`es from this canvas.\n","3. Select the anchor sizes which will match the problem the best.\n","\n","Selected anchor sizes should be stored in a sensible manner in `ANCHOR_SIZES` list."]},{"cell_type":"code","metadata":{"id":"KWLTAoFwkEnu","executionInfo":{"status":"ok","timestamp":1621284038035,"user_tz":-120,"elapsed":3971,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["# TODO\n","\n","widths = []\n","heighs = []\n","for i in range(1000):\n","  canvas =  get_random_canvas()\n","  #canvas.plot()\n","  for box in canvas.boxes:\n","    widths.append(box.x_diff)\n","    heighs.append(box.y_diff)\n","\n","ANCHOR_SIZES = [16,19]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhZMPbY55dSz","executionInfo":{"status":"ok","timestamp":1621284038037,"user_tz":-120,"elapsed":3968,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}},"outputId":"bf48ba96-845d-4aa4-bf57-bbeaa507ec58"},"source":["import numpy as np\n","from scipy.stats import mode\n","print(mode(widths))\n","print(mode(heighs))\n","print(np.mean(widths))\n","print(np.mean(heighs)) #todo z zajeć kmeans"],"execution_count":6,"outputs":[{"output_type":"stream","text":["ModeResult(mode=array([19]), count=array([4062]))\n","ModeResult(mode=array([19]), count=array([1146]))\n","18.712151616499444\n","14.80891861761427\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rZJsKiU6eSa","executionInfo":{"status":"ok","timestamp":1621284038038,"user_tz":-120,"elapsed":3963,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}},"outputId":"5cb5b244-7b91-4729-d8bb-9f6ae4f7951a"},"source":["np.histogram(heighs, bins=np.arange(21))"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([   0,    0,    0,   38,   31,   89,   24,   49,   35,  155,   62,\n","         373,  152,  621,  176,  667,  176,  544,  147, 1146]),\n"," array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n","        17, 18, 19, 20]))"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"3kARS2XYFsi_"},"source":["$n in \\{0,1, 32 -1}"]},{"cell_type":"markdown","metadata":{"id":"_GpJZUkDGJSi"},"source":["### Model building (1pt)\n","\n","\n","One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:\n","\n","#### $\\texttt{backbone}$:\n","\n","Backbone model should accept a `MnistCanvas` instance and output a tensor with shape $(1, \\frac{128}{2^k}, \\frac{128}{2^k}, filters)$ where $k \\in {2, 3, 4}$ and $filters$ should be selected by student. We will later relate to selected $k$ as a stride. You can use a pretrained backbone.\n","\n","#### $\\texttt{anchors}$:\n","\n","List of `MnistBox`es where each box:\n","\n","- should have size of one of selected `ANCHOR_SIZES`,\n","- should have center coordinates on canvas of type $\\left(\\left(m + \\frac{1}{2}\\right) * 2^k, \\left(n + \\frac{1}{2}\\right) * 2^k\\right)$  for $m, n \\in \\{0, 1, \\dots, \\frac{128}{2^k} - 1\\}$.\n","\n","`MnistBox` with anchor should have an attribute `class_nb` set to `None`.\n","\n","#### $\\texttt{classificationHead}$:\n","\n","$\\texttt{classificationHead}$ should accept backbone output as an input and output `classification_output` tensor of shape $(len(\\texttt{anchors)}, 10)$ where the value $ch_{i, j}$ which is the value of i-th row and j-th column has a property that $sigmoid(ch_{i, j})$ is a probability that i-th anchor from $\\texttt{anchors}$ overlaps significantly with some canvas `GTBox: MnistBox` with a digit of class $j$(ground truth box).\n","\n","#### $\\texttt{boxRegressionHead}$:\n","\n","$\\texttt{boxRegressionHead}$ should accept backbone output as an input and output `box_regression_output` tensor of shape $(len(\\texttt{anchors)}, 4)$ where the value $br_{i}$ which is the value of i-th row has a property that if there is a ground truth digit box $\\texttt{GTBox}$ - significantly overlapping with $\\texttt{anchor}[i]$ then the following properties hold:\n","\n","$$\\texttt{GTBox.x_min} = \\texttt{anchor[i].x_min} + br_{i, 0},$$ \n","$$\\texttt{GTBox.x_max} = \\texttt{anchor[i].x_max} + br_{i, 1},$$ \n","$$\\texttt{GTBox.y_min} = \\texttt{anchor[i].y_min} + br_{i, 2},$$ \n","$$\\texttt{GTBox.y_max} = \\texttt{anchor[i].y_max} + br_{i, 3}.$$ \n","\n","### Model output\n","\n","Model should output `DigitDetectionModelOutput` class defined below.\n","\n","#### Comment on _significant overlap_:\n","\n","The meaning of significant overlap will be described later."]},{"cell_type":"code","metadata":{"id":"r-SkvW3RA1FF","executionInfo":{"status":"ok","timestamp":1621284038039,"user_tz":-120,"elapsed":3959,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["import torch.nn as nn\n","\n","class NetConv(nn.Module):\n","    def __init__(self):\n","        super(NetConv, self).__init__()\n","        modules = []\n","\n","        modules.append(nn.Conv2d(1, 16, 3, padding=1))\n","        modules.append(nn.BatchNorm2d(16))\n","        modules.append(nn.ReLU())\n","        modules.append(nn.MaxPool2d(2))\n","\n","        modules.append(nn.Conv2d(16, 50, 3, padding=1))\n","        modules.append(nn.BatchNorm2d(50))\n","        modules.append(nn.ReLU())\n","        modules.append(nn.MaxPool2d(2))\n","\n","        self.layers = nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","\n","        return x\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGVqk8-lhTa0","executionInfo":{"status":"ok","timestamp":1621284038039,"user_tz":-120,"elapsed":3955,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["\n","class Reshape(nn.Module):\n","    def __init__(self, *args):\n","        super(Reshape, self).__init__()\n","        self.shape = args\n","\n","    def forward(self, x):\n","        return x.view((x.shape[0],) + self.shape)\n","\n","class ClassificationHead(nn.Module):\n","    def __init__(self, anchors_number):\n","        super(ClassificationHead, self).__init__()\n","        modules = []\n","        modules.append(nn.Flatten())\n","        modules.append(nn.Linear(32*32*50, anchors_number*10))\n","        modules.append(Reshape(anchors_number,10))\n","\n","\n","        self.layers = nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","\n","        return x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q-gRSg8lb4N","executionInfo":{"status":"ok","timestamp":1621284038040,"user_tz":-120,"elapsed":3953,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["class BoxRegressionHead(nn.Module):\n","    def __init__(self, anchors_number):\n","        super(BoxRegressionHead, self).__init__()\n","        modules = []\n","        modules.append(nn.Flatten())\n","        modules.append(nn.Linear(32*32*50, anchors_number*4))\n","        modules.append(Reshape(anchors_number, 4))\n","        self.layers = nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","\n","        return x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXco8riNGHhl","executionInfo":{"status":"ok","timestamp":1621284038241,"user_tz":-120,"elapsed":4150,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["class DigitDetectionModelOutput:\n","\n","    def __init__(\n","        self,\n","        anchors: List[MnistBox],\n","        classification_output: torch.Tensor,\n","        box_regression_output: torch.Tensor,\n","    ):\n","        self.anchors = anchors\n","        self.classification_output = classification_output\n","        self.box_regression_output = box_regression_output\n","\n","\n","class DigitDetectionModel(torch.nn.Module):\n","    # Should use ANCORS_SIZES\n","    anchors = []\n","    def __init__(\n","        self,\n","    ):\n","        for n in range(0, 128//4-1):\n","          for m in range(0, 128//4-1):\n","              self.anchors.append(MnistBox(m*4 - ANCHOR_SIZES[0]/2, n*4 - ANCHOR_SIZES[1]/2, m*4 + ANCHOR_SIZES[1]/2, n*4+ANCHOR_SIZES[0]/2))\n","        \n","        super().__init__()\n","\n","        #backend\n","        self.backend = NetConv()\n","        \n","        #heads\n","        self.box_reg = BoxRegressionHead(anchors_number=len(self.anchors))\n","        self.classification = ClassificationHead(anchors_number=len(self.anchors))\n","\n","\n","    def forward(self, x: MnistCanvas) -> DigitDetectionModelOutput: #dlaczego canvas a nie tensor? utrudnia robienie batchy\n","        #poprawka z canvasu na tensor\n","        x = x.get_torch_tensor()\n","        #backend\n","        x = self.backend(x)\n","\n","        #heads\n","        box_regression_output = self.box_reg(x.permute(0,2,3,1))\n","        classification_output = self.classification(x.permute(0,2,3,1))\n","\n","\n","\n","        return DigitDetectionModelOutput(self.anchors, classification_output, box_regression_output)\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnZu3sdhTfTJ"},"source":["### Significant overlap (1pt)\n","\n","In order to manage definition of a _significant overlap_ student should implement the `TargetDecoder`. `TargetDecoder` have two methods:\n","\n","### - `get_targets`\n","\n","This method accepts a `canvas: MnistCanvas` with boxes later refered to as `gt_boxes: List[MnistBox]`, `anchors: List[MnistBox]` of model anchors and `iou_threshold: float`. Its output should be `DigitDetectionModelTarget` instance with the following attributes:\n","\n","- `classification_target` - a tensor of shape $(len(anchors), 10)$,\n","- `box_regression_target` - a tensor of shape $(len(anchors), 4)$,\n","- `matched_anchors` - a list of indices anchors matched (see definition below).\n","\n","The output attributes should be computed in a following manner: \n","\n","1. All of the outputs of the output tensors should be `0` except the case presented in the point 2.\n","1. if for anchor `anchors[i]` there exist at least one `gt_box` from `gt_boxes` with `iou` overlap greater than `iou_threshold` then let `gt_best` be the one with the greatest `iou` overlap (ties resolved randomly). Then `box_regression_target` should encode the bounding box correction, namely:\n","\n","$$\\texttt{box_regression_target}[i, 0] = \\texttt{gt_best.x_min} - \\texttt{anchor[i].x_min}$$ \n","$$\\texttt{box_regression_target}[i, 1] = \\texttt{gt_best.x_max} - \\texttt{anchor[i].x_max}$$ \n","$$\\texttt{box_regression_target}[i, 2] = \\texttt{gt_best.y_min} - \\texttt{anchor[i].y_min}$$ \n","$$\\texttt{box_regression_target}[i, 3] = \\texttt{gt_best.y_max} - \\texttt{anchor[i].y_max}$$,\n","\n","and `classification_target` should encode the class of matched `gt_best`, namely:\n","\n","$$\\texttt{class_targets}[i, \\texttt{gt_best.class_nb}] = 1.$$\n","\n","Moreover - the `anchor[i]` is considered to be _matched_ with some ground truth box so index `i` should be in `matched_anchors` list.\n","\n","The output should be packed into `DigitDetectionModelTarget` class defined below.\n","\n","_Hint_: note that there might be cases when no anchor is matched. What does it mean about your anchors?\n","\n","### - `get_predictions`\n","\n","This method should decode the `DigitDetectionModelOutput` to set of final boxes\n","predictions. We leave the way of selecting the predictions to students.\n","\n","_HINT_: we definitely advise to use `torchvision.ops.nms` function.\n","\n"]},{"cell_type":"code","metadata":{"id":"WWcthFZQquNo","executionInfo":{"status":"ok","timestamp":1621284038242,"user_tz":-120,"elapsed":4147,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["class DigitDetectionModelTarget:\n","\n","    def __init__(\n","        self,\n","        classification_target: torch.Tensor,\n","        box_regression_target: torch.Tensor,\n","        matched_anchors: List[int],\n","    ):\n","        self.classification_target = classification_target\n","        self.box_regression_target = box_regression_target\n","        self.matched_anchors = matched_anchors\n","\n","import torchvision\n","\n","class TargetDecoder:\n","\n","    def get_targets(\n","        self,\n","        canvas: MnistCanvas,\n","        anchors: List[MnistBox],\n","        iou_threshold: float=0.5,\n","        nb_of_classes: int = 10,\n","    ) -> DigitDetectionModelTarget:\n","        matched_anchors = []\n","        classification_target = torch.zeros([len(anchors), nb_of_classes])\n","        box_regression_target = torch.zeros([len(anchors), 4])\n","        for i in (range(len(anchors))):\n","          accept = []\n","          for box_a in canvas.boxes:\n","            if box_a.iou_with(anchors[i]) >= iou_threshold:\n","              accept.append({'box':box_a, 'iou': box_a.iou_with(anchors[i])})\n","              matched_anchors.append(i)\n","          #pobaramy indeks najlepszego iou\n","          if len(accept) > 0:\n","            naj_ind = 0\n","            for j in range(len(accept)):\n","                if accept[j]['iou'] > accept[naj_ind]['iou']:\n","                  naj_ind = j\n","            #anchor cos ma \n","            classification_target[naj_ind, accept[naj_ind]['box'].class_nb] = 1\n","            box_regression_target[naj_ind, 0] = accept[naj_ind]['box'].x_min - anchors[i].x_min\n","            box_regression_target[naj_ind, 1] = accept[naj_ind]['box'].x_max - anchors[i].x_max\n","            box_regression_target[naj_ind, 2] = accept[naj_ind]['box'].y_min - anchors[i].y_min\n","            box_regression_target[naj_ind, 3] = accept[naj_ind]['box'].y_max - anchors[i].y_max\n","\n","\n","        return DigitDetectionModelTarget(classification_target, box_regression_target, matched_anchors)\n","\n","\n","                        \n","\n","    def get_predictions(\n","        self,\n","        model_output: DigitDetectionModelOutput,\n","    ) -> List[MnistBox]:\n","        scores = torch.zeros(len(model_output.anchors)).to(DEVICE)\n","        boxes = torch.zeros([len(model_output.anchors), 4]).to(DEVICE)\n","        scores,_ = torch.max(model_output.classification_output[0], dim=1)\n","        print(scores.shape)\n","        exit(-1)\n","        for i in range(len(model_output.anchors)):\n","          boxes[i,0] = model_output.anchors[i].x_min - model_output.box_regression_output[0][i][0]\n","          boxes[i,1] = model_output.anchors[i].y_min - model_output.box_regression_output[0][i][2]\n","          boxes[i,2] = model_output.anchors[i].x_max - model_output.box_regression_output[0][i][1]\n","          boxes[i,3] = model_output.anchors[i].y_max - model_output.box_regression_output[0][i][3]\n","\n","        indexes = torchvision.ops.nms(boxes, scores, 0.5)\n","        out_list = []\n","        for index in indexes:\n","          out_list.append(MnistBox(x_min=boxes[index,0],x_max =boxes[index,2],y_min=boxes[index,1], y_max=boxes[index,3], class_nb=torch.argmax(model_output.classification_output[0][index])))\n","\n","        return out_list\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Fj8U-8Q67I3"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"xA1Nvz6jagyP"},"source":["### Metrics (0.5pt)\n","\n","## Retina Loss\n","As a loss function one should implement the variant of Retina Loss. It should be computed in a following manner:\n","\n","`compute_loss`: \n","\n","This method accepts:\n","- `DigitDetectionModelTarget`,\n","- `DigitDetectionModelOutput`,\n","\n","and computes a loss which is a sum of a:\n","- `torch.nn.SmoothL1Loss` between boxes predictions and targets averaged only over matched anchors,\n","- `torchvision.ops.sigmoid_focal_loss` between classes predictions and targets averaged only over matched anchors.\n","\n","One can either use `torch` default parameters for this losses or try to tune them.\n","\n","If there are no matched anchors - a loss should return `None`. Remember to handle this case separately in your training loop. What does the occurence of this case means about your anchors?\n","\n","## Digit Accuracy\n","\n","This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]` obtained using `TargetDecoder.get_predictions` method and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n","\n","- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class and `iou` overlap greater than `0.5`,\n","- the number of `canvas` boxes match `len(predicted_boxes)`.\n","\n","The model shoud output `1` if there is a matching and `0` otherwise.\n"]},{"cell_type":"code","metadata":{"id":"rffXn9CQwVu4","executionInfo":{"status":"ok","timestamp":1621284038243,"user_tz":-120,"elapsed":4145,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["import torch\n","import torchvision \n","from torch.nn import SmoothL1Loss\n","from torchvision.ops import sigmoid_focal_loss \n","\n","class RetinaLoss:\n","\n","    def compute_loss(\n","        self,\n","        model_output: DigitDetectionModelOutput,\n","        model_target: DigitDetectionModelTarget,\n","    ) -> Optional[torch.Tensor]: \n","        loss_box_regression = 0\n","        loss_classification = 0 \n","        #for anchor_index in model_target.matched_anchors: @TODO do zmiany na wielu batch.\n","        test = SmoothL1Loss()\n","        loss_box_regression += test(model_output.box_regression_output[0].to(DEVICE), model_target.box_regression_target.to(DEVICE))\n","        loss_classification += sigmoid_focal_loss(model_output.classification_output[0].to(DEVICE), model_target.classification_target.to(DEVICE), reduction='mean') \n","\n","        if len(model_target.matched_anchors) == 0:\n","          return None\n","\n","        return (loss_box_regression + loss_classification)* model_output.classification_output.shape[1] / len(model_target.matched_anchors)\n","\n","\n","class DigitAccuracy:\n","\n","    def compute_metric(\n","        self,\n","        predicted_boxes: List[MnistBox],\n","        canvas: MnistCanvas,\n","    ):\n","        for box in canvas.boxes:\n","          find = False\n","          #bijection\n","          for pbox in predicted_boxes:\n","            if box.iou_with(pbox) > 0.5 and box.class_nb == pbox.class_nb:\n","              if find: #not injection\n","                return 0\n","              find = True\n","        if len(canvas.boxes) == len(predicted_boxes):\n","          return 1\n","        # not bijection\n","        return 0\n","\n","              "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"tvCGYWiWcvUD","executionInfo":{"status":"ok","timestamp":1621284038244,"user_tz":-120,"elapsed":4143,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}}},"source":["\n","import torch.optim as optim\n","\n","def train():\n","        model = DigitDetectionModel()\n","        model.to(DEVICE)\n","\n","        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","        rloss = RetinaLoss()\n","        acc = DigitAccuracy()\n","        target = TargetDecoder()\n","        \n","        acc_add = 0\n","\n","        for epoch in range(100):\n","            canvas = get_random_canvas()\n","            #canvas_tensor = canvas.get_torch_tensor()\n","            #canvas_tensor = canvas_tensor.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(canvas)\n","            acc_value = acc.compute_metric(target.get_predictions(outputs), canvas)\n","\n","            targets = target.get_targets(canvas, outputs.anchors, iou_threshold=0.5, nb_of_classes=10)\n","            loss = rloss.compute_loss(outputs, targets)\n","\n","            print('treningowe accuracy:', acc)\n","            print('loss', loss)\n","            loss.backward()\n","            print(\"ayay\")\n","            optimizer.step()\n","            print(\"no tu nas raczej nie bedzie\")\n","            acc_add+=acc_value\n","            \n","                    \n","\n","            print('Accuracy of the network on the {} test images: {} %'.format(\n","                1, acc_add))"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvKOaKS5fJjo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621284149008,"user_tz":-120,"elapsed":114898,"user":{"displayName":"Michał Raszkowski","photoUrl":"","userId":"07581024195908049954"}},"outputId":"0268b362-ecec-4c3a-bb1b-a9c9760111f1"},"source":["train()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(24.6750, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(16.6964, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(14.9830, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(26.5060, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(13.6267, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(16.3306, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(23.5118, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.4628, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(9.3424, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(10.0707, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(13.8363, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(5.4165, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(5.1395, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(8.0338, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.7674, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.9801, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(7.3486, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(10.3629, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.2892, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(5.4815, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.7712, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.6977, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.3142, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.4415, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.5812, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.4639, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.2848, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(28.9454, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.0273, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.3472, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.7390, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.0263, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.6715, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.0380, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.7176, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.7212, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.8257, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(5.3471, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.3053, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.4021, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.2916, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.9235, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.2806, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.3576, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.6858, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(4.1499, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(6.6793, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.3424, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.1576, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(3.3457, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.5347, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.7879, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.5871, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.5762, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.6737, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.1266, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.5691, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.1575, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.0855, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3733, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(5.7066, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.9384, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3780, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3529, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9498, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.8369, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3317, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.0381, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.0877, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.6044, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.0192, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.2053, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.0935, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.1071, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.6235, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9019, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.5716, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.7187, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.7476, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3159, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9257, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.7327, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.5720, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.5268, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.4031, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.2262, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3993, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.4521, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.6339, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.9125, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(2.4733, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.7262, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.3369, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.8584, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9583, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9863, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.1105, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.9512, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(1.2617, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n","torch.Size([961])\n","treningowe accuracy: <__main__.DigitAccuracy object at 0x7fc89efdee10>\n","loss tensor(0.6092, device='cuda:0', grad_fn=<DivBackward0>)\n","ayay\n","no tu nas raczej nie bedzie\n","Accuracy of the network on the 1 test images: 0 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"15n5w-hhvRbS"},"source":["### Train your model (1pt)\n","\n","One should use all classes defined above to train the model.\n","\n","A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data.\n","\n","Plot example results of matched and mismatched predictions (0.5pt).\n","\n","_Hint:_ note that all classes defined above accept only a single canvas as their inputs. Can we extend a training loop to use it for a training on batches?\n","\n","Have fun!"]}]}